{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验9：人工神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 一：实验目的和要求\n",
    "    1．掌握利用Python实现神经网络的基本算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二：内容和方法\n",
    "- 使用BP神经网络预测马是否患有疝气病。训练数据和测试数据分别为 7_horseColicTraining.txt 和 7_horseColicTest.txt， 数据有22列，前21列是一些特征，最后一列是病症数据。设计合适的神经网络，用训练样本训练，并使得测试样本的错误率控制在30%以下。\n",
    "- 实验步骤：<br>\n",
    "    1.用np.loadtxt读取文本文件为变量。<br>\n",
    "    2.用 sklearn.decomposition模块中PCA对象，根据训练样本拟合出PCA降到5维的模型，然后分别对训练样本和测试样本做PCA变换。<br>\n",
    "    3.设置神经网络的参数：网络层数、每层的节点数、学习率、迭代次数等。<br>\n",
    "    4.用训练样本训练神经网络。<br>\n",
    "    5.用测试样本计算错误率。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.用np.loadtxt读取文本文件为变量。\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# 获取数据并分为训练集与测试集\n",
    "a = np.loadtxt('D:/root/Desktop/各科/kk/实验9：人工神经网络/7_horseColicTraining.txt',delimiter='\\t')\n",
    "trainingSet = a[:,0:-1]\n",
    "trainingLabels = a[:,-1]\n",
    "\n",
    "b = np.loadtxt('D:/root/Desktop/各科/kk/实验9：人工神经网络/7_horseColicTest.txt',delimiter='\\t')\n",
    "testSet = b[:,0:-1]\n",
    "testLabels = b[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2. ,  1. , 38.5, ...,  8.4,  0. ,  0. ],\n",
       "        [ 1. ,  1. , 39.2, ..., 85. ,  2. ,  2. ],\n",
       "        [ 2. ,  1. , 38.3, ...,  6.7,  0. ,  0. ],\n",
       "        ...,\n",
       "        [ 1. ,  1. , 37.5, ...,  6.8,  0. ,  0. ],\n",
       "        [ 1. ,  1. , 36.5, ...,  6. ,  3. ,  3.4],\n",
       "        [ 1. ,  1. , 37.2, ..., 62. ,  1. ,  1. ]]),\n",
       " array([0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "        0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
       "        1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
       "        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1.,\n",
       "        0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
       "        0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
       "        1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0.]),\n",
       " array([[ 2. ,  1. , 38.5, ...,  6.3,  0. ,  0. ],\n",
       "        [ 2. ,  1. , 37.6, ...,  6.3,  1. ,  5. ],\n",
       "        [ 1. ,  1. , 37.7, ..., 70. ,  3. ,  2. ],\n",
       "        ...,\n",
       "        [ 1. ,  1. , 38. , ..., 65. ,  3. ,  2. ],\n",
       "        [ 2. ,  1. , 38. , ...,  5.8,  0. ,  0. ],\n",
       "        [ 2. ,  1. , 37.6, ...,  6. ,  0. ,  0. ]]),\n",
       " array([1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "        0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
       "        0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainingSet,trainingLabels,testSet,testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((299, 5), (67, 5))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2.用 sklearn.decomposition模块中PCA对象，根据训练样本拟合出PCA降到5维的模型，然后分别对训练样本和测试样本做PCA变换。\n",
    "from sklearn.decomposition import PCA\n",
    "model = PCA(n_components=5).fit(trainingSet)\n",
    "trainingSet_pca = model.transform(trainingSet)\n",
    "testSet_pca = model.transform(testSet)\n",
    "\n",
    "trainingSet_pca.shape,testSet_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.设置神经网络的参数：网络层数、每层的节点数、学习率、迭代次数等。\n",
    "layer =[trainingSet.shape[1],3,1] #设置层数和节点\n",
    "Lambda = 0.005 # 正则化系数\n",
    "alpha = 0.6 # 学习速率\n",
    "num_passes = 1000 # 迭代次数\n",
    "m = len(trainingSet) # 样本数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.用训练样本训练神经网络。\n",
    "\n",
    "from numpy import random\n",
    "# 建立网络\n",
    "# 网络采用列表存储每层的网络结构,网络的层数和各层节点数都可以自由设定\n",
    "b = [] # 偏置元,共layer-1个元素,b[0]代表第一个隐藏层的偏置元(向量形式)\n",
    "W = []\n",
    "for i in range(len(layer)-1):\n",
    "    W.append(random.random(size = (layer[i+1],layer[i]))) # W[i]表示网络第i层到第i+1层的转移矩阵(NumPy数组),输入层是第0层\n",
    "    b.append(np.array([0.1]*layer[i+1]))  # 偏置元,b[i]的规模是1*第i+1个隐藏层节点数\n",
    "a = [np.array(0)]*(len(W)+1) # a[0] = x,即输入,a[1]=f(z[0]),a[len(W)+1] = 最终输出\n",
    "z = [np.array(0)]*len(W) # 注意z[0]表示是网络输入层的输出，即未被激活的第一个隐藏层\n",
    "\n",
    "W = np.array(W)\n",
    "\n",
    "def costfunction(predict,labels):\n",
    "    # 不加入正则化项的代价函数\n",
    "    # 输入参数格式为numpy的向量\n",
    "    return sum((predict - labels)**2)\n",
    " \n",
    "def error_rate(predict,labels):\n",
    "    # 计算错误率,针对二分类问题,分类标签为0或1\n",
    "    # 输入参数格式为numpy的向量\n",
    "    error =0.0\n",
    "    for i in range(len(predict)):\n",
    "        predict[i] = round(predict[i]) \n",
    "        if predict[i]!=labels[i]:\n",
    "            error+=1\n",
    "    return error/len(predict)\n",
    " \n",
    "def sigmoid(z):  # 激活函数sigmoid\n",
    "    return 1/(1+np.exp(-z))\n",
    "def diff_sigmoid(z): # 激活函数sigmoid的导数\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "activation_function = sigmoid  # 设置激活函数\n",
    "diff_activation_function = diff_sigmoid # 设置激活函数的导数\n",
    "\n",
    "\n",
    "# 开始训练BP神经网络\n",
    "a[0] = np.array(trainingSet).T # 改一列为一个样本，一行代表一个特征\n",
    "y = np.array(trainingLabels)\n",
    "\n",
    "for v in range(num_passes):\n",
    "    # 前向传播\n",
    "    for i in range(len(W)):\n",
    "        z[i] = np.dot(W[i],a[i])\n",
    "        for j in range(m):\n",
    "            z[i][:,j]+=b[i] # 加上偏置元\n",
    "        a[i+1] = activation_function(z[i]) # 激活节点\n",
    "\n",
    "    predict = a[-1][0] # a[-1]是输出层的结果,即为预测值\n",
    "\n",
    "    # 反向传播\n",
    "    delta = [np.array(0)]*len(W) # delta[0]是第一个隐藏层的残差，delta[-1]是输出层的残差\n",
    "\n",
    "    # 计算输出层的残差\n",
    "    delta[-1] = -(y-a[-1])*diff_activation_function(z[-1])\n",
    "\n",
    "    # 计算第二层起除输出层外的残差\n",
    "    for i in range(len(delta)-1):\n",
    "        delta[-i-2] = np.dot(W[-i-1].T,delta[-i-1])*diff_activation_function(z[-i-2]) # 这里是倒序遍历\n",
    "        # 设下标-i-2代表第L层，则W[-i-1]是第L层到L+1层的转移矩阵，delta[-i-1]是第L+1层的残差，而z[-i-2]是未激活的第L层\n",
    "\n",
    "    # 计算最终需要的偏导数值\n",
    "    delta_w = [np.array(0)]*len(W)\n",
    "    delta_b = [np.array(0)]*len(W)\n",
    "    for i in range(len(W)):\n",
    "        # 使用矩阵运算简化公式,下面2行代码已将全部样本反向传播得到的偏导数值求和\n",
    "        delta_w[i] = np.dot(delta[i],a[i].T) \n",
    "        delta_b[i] = np.sum(delta[i],axis=1) \n",
    "\n",
    "    # 更新权重参数\n",
    "    for i in range(len(W)):\n",
    "        W[i] -= alpha*(Lambda*W[i]+delta_w[i]/m)\n",
    "        b[i] -= alpha/m*delta_b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练样本错误率: 0.2809364548494983\n"
     ]
    }
   ],
   "source": [
    "# 5.用测试样本计算错误率。\n",
    "print('训练样本错误率:',error_rate(predict,np.array(trainingLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试样本错误率: 0.26865671641791045\n"
     ]
    }
   ],
   "source": [
    "# 使用测试集测试网络\n",
    "a[0] = np.array(testSet).T # 改一列为一个样本，一行代表一个特征\n",
    "# 前向传播\n",
    "m = len(testSet)\n",
    "for i in range(len(W)):\n",
    "    z[i] = np.dot(W[i],a[i])\n",
    "    for j in range(m):\n",
    "        z[i][:,j]+=b[i].T[0]\n",
    "    a[i+1] = activation_function(z[i])\n",
    "predict = a[-1][0]\n",
    "\n",
    "print('测试样本错误率:',error_rate(predict,np.array(testLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
